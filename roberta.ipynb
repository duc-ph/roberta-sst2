{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 23:47:48.243821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-18 23:47:48.256288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737244068.277305   52911 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737244068.285706   52911 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 23:47:48.311681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR, SequentialLR\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = [[] for _ in range(6)]\n",
    "for split, xb, yb in zip([train_data, validation_data, test_data], [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    for observation in split:\n",
    "        sentence, label, _ = observation.values()\n",
    "        xb.append(sentence)\n",
    "        yb.append(label)\n",
    "    \n",
    "    xb = tokenizer(xb, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(X_train, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "X_val = tokenizer(X_val, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "X_test = tokenizer(X_test, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = {\n",
    "            'input_ids': self.X['input_ids'][idx, :].to(device),\n",
    "            'attention_mask': self.X['attention_mask'][idx, :].to(device)\n",
    "        }\n",
    "        label = self.y[idx]\n",
    "        y = torch.Tensor([1.0, 0.0]) if label == 0 else torch.Tensor([0.0, 1.0])\n",
    "        y = y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SST2Dataset(X_train, y_train)\n",
    "val_dataset = SST2Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 1.0e-05, batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/4210], Loss: 0.7475, LR: 5.0e-07\n",
      "Epoch [1/10], Step [201/4210], Loss: 0.6733, LR: 1.3e-06\n",
      "Epoch [1/10], Step [401/4210], Loss: 0.7416, LR: 2.0e-06\n",
      "Epoch [1/10], Step [601/4210], Loss: 0.6478, LR: 2.8e-06\n",
      "Epoch [1/10], Step [801/4210], Loss: 0.5114, LR: 3.5e-06\n",
      "Epoch [1/10], Step [1001/4210], Loss: 0.4141, LR: 4.3e-06\n",
      "Epoch [1/10], Step [1201/4210], Loss: 0.4002, LR: 5.0e-06\n",
      "Epoch [1/10], Step [1401/4210], Loss: 0.1708, LR: 5.8e-06\n",
      "Epoch [1/10], Step [1601/4210], Loss: 0.4014, LR: 6.5e-06\n",
      "Epoch [1/10], Step [1801/4210], Loss: 0.3184, LR: 7.3e-06\n",
      "Epoch [1/10], Step [2001/4210], Loss: 0.1687, LR: 8.0e-06\n",
      "Epoch [1/10], Step [2201/4210], Loss: 0.1916, LR: 8.8e-06\n",
      "Epoch [1/10], Step [2401/4210], Loss: 0.2698, LR: 9.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [2601/4210], Loss: 0.4231, LR: 1.0e-05\n",
      "Epoch [1/10], Step [2801/4210], Loss: 0.1670, LR: 9.9e-06\n",
      "Epoch [1/10], Step [3001/4210], Loss: 0.0703, LR: 9.9e-06\n",
      "Epoch [1/10], Step [3201/4210], Loss: 0.2558, LR: 9.8e-06\n",
      "Epoch [1/10], Step [3401/4210], Loss: 0.4270, LR: 9.8e-06\n",
      "Epoch [1/10], Step [3601/4210], Loss: 0.3537, LR: 9.7e-06\n",
      "Epoch [1/10], Step [3801/4210], Loss: 0.2602, LR: 9.7e-06\n",
      "Epoch [1/10], Step [4001/4210], Loss: 0.1713, LR: 9.6e-06\n",
      "Epoch [1/10], Step [4201/4210], Loss: 0.1450, LR: 9.6e-06\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.0124\n",
      "    Validation loss: 0.2148\n",
      "    Accuracy: 0.9193\n",
      "========================================\n",
      "Epoch [2/10], Step [1/4210], Loss: 0.2313, LR: 9.6e-06\n",
      "Epoch [2/10], Step [201/4210], Loss: 0.8936, LR: 9.5e-06\n",
      "Epoch [2/10], Step [401/4210], Loss: 0.1684, LR: 9.5e-06\n",
      "Epoch [2/10], Step [601/4210], Loss: 0.1246, LR: 9.4e-06\n",
      "Epoch [2/10], Step [801/4210], Loss: 0.4913, LR: 9.4e-06\n",
      "Epoch [2/10], Step [1001/4210], Loss: 0.1045, LR: 9.3e-06\n",
      "Epoch [2/10], Step [1201/4210], Loss: 0.1437, LR: 9.3e-06\n",
      "Epoch [2/10], Step [1401/4210], Loss: 0.0328, LR: 9.2e-06\n",
      "Epoch [2/10], Step [1601/4210], Loss: 0.1922, LR: 9.2e-06\n",
      "Epoch [2/10], Step [1801/4210], Loss: 0.3842, LR: 9.1e-06\n",
      "Epoch [2/10], Step [2001/4210], Loss: 0.1478, LR: 9.1e-06\n",
      "Epoch [2/10], Step [2201/4210], Loss: 0.1678, LR: 9.0e-06\n",
      "Epoch [2/10], Step [2401/4210], Loss: 0.1260, LR: 9.0e-06\n",
      "Epoch [2/10], Step [2601/4210], Loss: 0.3924, LR: 8.9e-06\n",
      "Epoch [2/10], Step [2801/4210], Loss: 0.3475, LR: 8.9e-06\n",
      "Epoch [2/10], Step [3001/4210], Loss: 0.2110, LR: 8.8e-06\n",
      "Epoch [2/10], Step [3201/4210], Loss: 0.2076, LR: 8.8e-06\n",
      "Epoch [2/10], Step [3401/4210], Loss: 0.0982, LR: 8.7e-06\n",
      "Epoch [2/10], Step [3601/4210], Loss: 0.3687, LR: 8.7e-06\n",
      "Epoch [2/10], Step [3801/4210], Loss: 0.0961, LR: 8.6e-06\n",
      "Epoch [2/10], Step [4001/4210], Loss: 0.2266, LR: 8.6e-06\n",
      "Epoch [2/10], Step [4201/4210], Loss: 0.3623, LR: 8.5e-06\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.2127\n",
      "    Validation loss: 0.2938\n",
      "    Accuracy: 0.8830\n",
      "========================================\n",
      "Epoch [3/10], Step [1/4210], Loss: 0.3341, LR: 8.5e-06\n",
      "Epoch [3/10], Step [201/4210], Loss: 0.3386, LR: 8.5e-06\n",
      "Epoch [3/10], Step [401/4210], Loss: 0.2744, LR: 8.4e-06\n",
      "Epoch [3/10], Step [601/4210], Loss: 0.2052, LR: 8.4e-06\n",
      "Epoch [3/10], Step [801/4210], Loss: 0.3543, LR: 8.3e-06\n",
      "Epoch [3/10], Step [1001/4210], Loss: 0.1336, LR: 8.3e-06\n",
      "Epoch [3/10], Step [1201/4210], Loss: 0.1907, LR: 8.2e-06\n",
      "Epoch [3/10], Step [1401/4210], Loss: 0.1402, LR: 8.2e-06\n",
      "Epoch [3/10], Step [1601/4210], Loss: 0.2132, LR: 8.1e-06\n",
      "Epoch [3/10], Step [1801/4210], Loss: 0.1292, LR: 8.1e-06\n",
      "Epoch [3/10], Step [2001/4210], Loss: 0.5241, LR: 8.0e-06\n",
      "Epoch [3/10], Step [2201/4210], Loss: 0.1139, LR: 8.0e-06\n",
      "Epoch [3/10], Step [2401/4210], Loss: 0.3725, LR: 7.9e-06\n",
      "Epoch [3/10], Step [2601/4210], Loss: 0.2154, LR: 7.9e-06\n",
      "Epoch [3/10], Step [2801/4210], Loss: 0.1378, LR: 7.8e-06\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "all_runs = []\n",
    "for batch_size in [16, 32]:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    warmup_ratio = 0.06\n",
    "    warmup_steps = warmup_ratio * total_steps\n",
    "    patience = 3 # early stopping after 3 epochs with no improvement\n",
    "    eval_iters = len(train_dataloader) // 10 # how many batch we will evaluate train loss on\n",
    "\n",
    "    for learning_rate in [1e-5, 2e-5, 3e-5]:\n",
    "        print('========= RUN PARAMETERS: ===============')\n",
    "        print(f'Learning rate: {learning_rate:.1e}, batch size: {batch_size}')\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = Adam(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.98),\n",
    "            weight_decay=0.1,\n",
    "            eps=1e-6\n",
    "        )\n",
    "\n",
    "        warmup_scheduler = LinearLR(\n",
    "            optimizer=optimizer,\n",
    "            start_factor=0.05,\n",
    "            end_factor=1.0,\n",
    "            total_iters=warmup_steps,\n",
    "        )\n",
    "        decay_scheduler = LinearLR(\n",
    "            optimizer=optimizer,\n",
    "            start_factor=1.0,\n",
    "            end_factor=0.0,\n",
    "            total_iters=total_steps - warmup_steps,\n",
    "        )\n",
    "\n",
    "        scheduler = SequentialLR(\n",
    "            optimizer=optimizer,\n",
    "            schedulers=[warmup_scheduler, decay_scheduler],\n",
    "            milestones=[warmup_steps]\n",
    "        )\n",
    "\n",
    "        min_val_loss = float('inf')\n",
    "        epoch_since_last_loss_improvement = 0\n",
    "\n",
    "        # training loop\n",
    "        train_losses, val_losses, accuracies = [], [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "                # forward\n",
    "                logits = model(**x).logits\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "                if batch_idx % 200 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "            \n",
    "            # get train loss over eval_iter batches\n",
    "            train_loss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xb, yb) in enumerate(train_dataloader):\n",
    "                    if batch_idx == eval_iters:\n",
    "                        break\n",
    "                    logits = model(**x).logits\n",
    "                    loss = F.cross_entropy(logits, y)\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "            train_loss /= eval_iters\n",
    "\n",
    "            # Get validation loss and accuracy\n",
    "            val_accuracy = 0\n",
    "            val_loss = 0\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (x, y) in enumerate(val_dataloader):\n",
    "                    logits = model(**x).logits\n",
    "                    softmax = F.softmax(logits, dim=1)\n",
    "                    loss = F.cross_entropy(logits, y)\n",
    "                    y_preds = torch.argmax(softmax, dim=1)\n",
    "                    y = torch.Tensor([x[1] for x in y]).to(device)\n",
    "\n",
    "                    accuracy = (torch.sum(y_preds == y)/len(y)).item()\n",
    "                    val_accuracy += accuracy\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy /= len(val_dataloader)\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] summary\")\n",
    "            print(f'    Train loss: {train_loss:.4f}')\n",
    "            print(f'    Validation loss: {val_loss:.4f}')\n",
    "            print(f'    Accuracy: {val_accuracy:.4f}')\n",
    "            print('========================================')\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                epoch_since_last_loss_improvement = 0\n",
    "            else:\n",
    "                epoch_since_last_loss_improvement += 1\n",
    "                if epoch_since_last_loss_improvement > patience:\n",
    "                    print(\"-------- Early stopping --------------\")\n",
    "                    break\n",
    "\n",
    "        all_runs.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'accuracies': accuracies,\n",
    "        })\n",
    "\n",
    "        print(f'Best accuracy is {max(accuracies):.4f} at epoch {accuracies.index(max(accuracies)) + 1}')\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 2e-05,\n",
       "  'batch_size': 64,\n",
       "  'train_losses': [0.2679, 0.2083, 0.2443],\n",
       "  'val_losses': [0.2092, 0.272, 0.3516],\n",
       "  'val_accuracy': [0.9292, 0.8882, 0.852]},\n",
       " {'learning_rate': 5e-06,\n",
       "  'batch_size': 64,\n",
       "  'train_losses': [0.354, 0.1906, 0.1685],\n",
       "  'val_losses': [0.198, 0.199, 0.2054],\n",
       "  'val_accuracy': [0.9321, 0.9292, 0.927]},\n",
       " {'initial learning_rate': 5e-06,\n",
       "  'stepLR gamma': 0.5,\n",
       "  'batch_size': 64,\n",
       "  'train_losses': [0.354, 0.1899, 0.1712],\n",
       "  'val_losses': [0.198, 0.199, 0.2005],\n",
       "  'val_accuracy': [0.9321, 0.9277, 0.9259]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    {\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 64,\n",
    "        'train_losses': [0.2679, 0.2083, 0.2443],\n",
    "        'val_losses': [0.2092, 0.2720, 0.3516],\n",
    "        'val_accuracy': [0.9292, 0.8882, 0.8520]\n",
    "    },\n",
    "\n",
    "    # unstable loss even in first epoch => decrease learning rate\n",
    "    {\n",
    "        'learning_rate': 5e-6,\n",
    "        'batch_size': 64,\n",
    "        'train_losses': [0.3540, 0.1906, 0.1685],\n",
    "        'val_losses': [0.1980, 0.1990, 0.2054],\n",
    "        'val_accuracy': [0.9321, 0.9292, 0.9270]\n",
    "    },\n",
    "    \n",
    "    # observe that loss fluctuates more in epoch 2 => implemented step learning rate, hopefully there will be gain in epochs 2 and 3\n",
    "    {\n",
    "        'initial learning_rate': 5e-6,\n",
    "        'stepLR gamma': 0.5, # decrease LR by half after each epoch\n",
    "        'batch_size': 64,\n",
    "        'train_losses': [0.3540, 0.1899, 0.1712],\n",
    "        'val_losses': [0.1980, 0.1990, 0.2005],\n",
    "        'val_accuracy': [0.9321, 0.9277, 0.9259]\n",
    "    } \n",
    "\n",
    "    # there is not much difference in the metrics of epoch 2 and 3. I think further decreasing gamma will not bring about much changes.\n",
    "    # I will now focus on increasing accuracy on epoch 1.\n",
    "    # We reduced the learning rate in one of the experiments above and got nice result. Now I will tune it down a bit further.\n",
    "\n",
    "\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
