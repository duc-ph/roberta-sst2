{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 09:04:23.619392: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 09:04:23.636603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737277463.653499  141800 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737277463.658090  141800 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 09:04:23.673063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR, LinearLR, SequentialLR\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from utils import get_loss_and_accuracy, SST2Dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = [[] for _ in range(6)]\n",
    "for split, x, y in zip([train_data, validation_data, test_data], [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    for observation in split:\n",
    "        sentence, label, _ = observation.values()\n",
    "        x.append(sentence)\n",
    "        y.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = [tokenizer(X, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device) for X in [X_train, X_val, X_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SST2Dataset(X_train, y_train)\n",
    "val_dataset = SST2Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, './datasets/train_dataset.pth')\n",
    "torch.save(val_dataset, './datasets/val_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 1.0e-05, batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/4210], Loss: 0.7475, LR: 5.0e-07\n",
      "Epoch [1/10], Step [201/4210], Loss: 0.6956, LR: 1.3e-06\n",
      "Epoch [1/10], Step [401/4210], Loss: 0.7214, LR: 2.0e-06\n",
      "Epoch [1/10], Step [601/4210], Loss: 0.6610, LR: 2.8e-06\n",
      "Epoch [1/10], Step [801/4210], Loss: 0.6856, LR: 3.5e-06\n",
      "Epoch [1/10], Step [1001/4210], Loss: 0.6795, LR: 4.3e-06\n",
      "Epoch [1/10], Step [1201/4210], Loss: 0.6862, LR: 5.0e-06\n",
      "Epoch [1/10], Step [1401/4210], Loss: 0.7135, LR: 5.8e-06\n",
      "Epoch [1/10], Step [1601/4210], Loss: 0.4193, LR: 6.5e-06\n",
      "Epoch [1/10], Step [1801/4210], Loss: 0.4360, LR: 7.3e-06\n",
      "Epoch [1/10], Step [2001/4210], Loss: 0.2258, LR: 8.0e-06\n",
      "Epoch [1/10], Step [2201/4210], Loss: 0.3720, LR: 8.8e-06\n",
      "Epoch [1/10], Step [2401/4210], Loss: 0.3038, LR: 9.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [2601/4210], Loss: 0.4430, LR: 1.0e-05\n",
      "Epoch [1/10], Step [2801/4210], Loss: 0.2578, LR: 9.9e-06\n",
      "Epoch [1/10], Step [3001/4210], Loss: 0.0949, LR: 9.9e-06\n",
      "Epoch [1/10], Step [3201/4210], Loss: 0.2585, LR: 9.8e-06\n",
      "Epoch [1/10], Step [3401/4210], Loss: 0.5005, LR: 9.8e-06\n",
      "Epoch [1/10], Step [3601/4210], Loss: 0.4587, LR: 9.7e-06\n",
      "Epoch [1/10], Step [3801/4210], Loss: 0.5679, LR: 9.7e-06\n",
      "Epoch [1/10], Step [4001/4210], Loss: 0.4998, LR: 9.6e-06\n",
      "Epoch [1/10], Step [4201/4210], Loss: 0.5385, LR: 9.6e-06\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.1188\n",
      "    Validation loss: 0.4090\n",
      "    Accuracy: 0.8057\n",
      "========================================\n",
      "Epoch [2/10], Step [1/4210], Loss: 0.3187, LR: 9.6e-06\n",
      "Epoch [2/10], Step [201/4210], Loss: 0.7634, LR: 9.5e-06\n",
      "Epoch [2/10], Step [401/4210], Loss: 0.2093, LR: 9.5e-06\n",
      "Epoch [2/10], Step [601/4210], Loss: 0.3806, LR: 9.4e-06\n",
      "Epoch [2/10], Step [801/4210], Loss: 0.4714, LR: 9.4e-06\n",
      "Epoch [2/10], Step [1001/4210], Loss: 0.2442, LR: 9.3e-06\n",
      "Epoch [2/10], Step [1201/4210], Loss: 0.3869, LR: 9.3e-06\n",
      "Epoch [2/10], Step [1401/4210], Loss: 0.4567, LR: 9.2e-06\n",
      "Epoch [2/10], Step [1601/4210], Loss: 0.4141, LR: 9.2e-06\n",
      "Epoch [2/10], Step [1801/4210], Loss: 0.4219, LR: 9.1e-06\n",
      "Epoch [2/10], Step [2001/4210], Loss: 0.4141, LR: 9.1e-06\n",
      "Epoch [2/10], Step [2201/4210], Loss: 0.4297, LR: 9.0e-06\n",
      "Epoch [2/10], Step [2401/4210], Loss: 0.5073, LR: 9.0e-06\n",
      "Epoch [2/10], Step [2601/4210], Loss: 0.5678, LR: 8.9e-06\n",
      "Epoch [2/10], Step [2801/4210], Loss: 0.3989, LR: 8.9e-06\n",
      "Epoch [2/10], Step [3001/4210], Loss: 0.5473, LR: 8.8e-06\n",
      "Epoch [2/10], Step [3201/4210], Loss: 0.5111, LR: 8.8e-06\n",
      "Epoch [2/10], Step [3401/4210], Loss: 0.4672, LR: 8.7e-06\n",
      "Epoch [2/10], Step [3601/4210], Loss: 0.4093, LR: 8.7e-06\n",
      "Epoch [2/10], Step [3801/4210], Loss: 0.2876, LR: 8.6e-06\n",
      "Epoch [2/10], Step [4001/4210], Loss: 0.4597, LR: 8.6e-06\n",
      "Epoch [2/10], Step [4201/4210], Loss: 0.3709, LR: 8.5e-06\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.3951\n",
      "    Validation loss: 0.4410\n",
      "    Accuracy: 0.7989\n",
      "========================================\n",
      "Epoch [3/10], Step [1/4210], Loss: 0.6250, LR: 8.5e-06\n",
      "Epoch [3/10], Step [201/4210], Loss: 0.4644, LR: 8.5e-06\n",
      "Epoch [3/10], Step [401/4210], Loss: 0.5012, LR: 8.4e-06\n",
      "Epoch [3/10], Step [601/4210], Loss: 0.4567, LR: 8.4e-06\n",
      "Epoch [3/10], Step [801/4210], Loss: 0.3835, LR: 8.3e-06\n",
      "Epoch [3/10], Step [1001/4210], Loss: 0.2691, LR: 8.3e-06\n",
      "Epoch [3/10], Step [1201/4210], Loss: 0.3893, LR: 8.2e-06\n",
      "Epoch [3/10], Step [1401/4210], Loss: 0.3879, LR: 8.2e-06\n",
      "Epoch [3/10], Step [1601/4210], Loss: 0.2475, LR: 8.1e-06\n",
      "Epoch [3/10], Step [1801/4210], Loss: 0.5029, LR: 8.1e-06\n",
      "Epoch [3/10], Step [2001/4210], Loss: 0.6893, LR: 8.0e-06\n",
      "Epoch [3/10], Step [2201/4210], Loss: 0.2955, LR: 8.0e-06\n",
      "Epoch [3/10], Step [2401/4210], Loss: 0.4973, LR: 7.9e-06\n",
      "Epoch [3/10], Step [2601/4210], Loss: 0.4803, LR: 7.9e-06\n",
      "Epoch [3/10], Step [2801/4210], Loss: 0.4947, LR: 7.8e-06\n",
      "Epoch [3/10], Step [3001/4210], Loss: 0.4313, LR: 7.8e-06\n",
      "Epoch [3/10], Step [3201/4210], Loss: 0.6144, LR: 7.7e-06\n",
      "Epoch [3/10], Step [3401/4210], Loss: 0.5704, LR: 7.7e-06\n",
      "Epoch [3/10], Step [3601/4210], Loss: 0.4957, LR: 7.6e-06\n",
      "Epoch [3/10], Step [3801/4210], Loss: 0.5422, LR: 7.6e-06\n",
      "Epoch [3/10], Step [4001/4210], Loss: 0.4616, LR: 7.5e-06\n",
      "Epoch [3/10], Step [4201/4210], Loss: 0.3678, LR: 7.4e-06\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.1545\n",
      "    Validation loss: 0.4600\n",
      "    Accuracy: 0.7830\n",
      "========================================\n",
      "Epoch [4/10], Step [1/4210], Loss: 0.6298, LR: 7.4e-06\n",
      "Epoch [4/10], Step [201/4210], Loss: 0.4229, LR: 7.4e-06\n",
      "Epoch [4/10], Step [401/4210], Loss: 0.4962, LR: 7.3e-06\n",
      "Epoch [4/10], Step [601/4210], Loss: 0.6141, LR: 7.3e-06\n",
      "Epoch [4/10], Step [801/4210], Loss: 0.6074, LR: 7.2e-06\n",
      "Epoch [4/10], Step [1001/4210], Loss: 0.3564, LR: 7.2e-06\n",
      "Epoch [4/10], Step [1201/4210], Loss: 0.4908, LR: 7.1e-06\n",
      "Epoch [4/10], Step [1401/4210], Loss: 0.4971, LR: 7.1e-06\n",
      "Epoch [4/10], Step [1601/4210], Loss: 0.6016, LR: 7.0e-06\n",
      "Epoch [4/10], Step [1801/4210], Loss: 0.5945, LR: 7.0e-06\n",
      "Epoch [4/10], Step [2001/4210], Loss: 0.4693, LR: 6.9e-06\n",
      "Epoch [4/10], Step [2201/4210], Loss: 0.2936, LR: 6.9e-06\n",
      "Epoch [4/10], Step [2401/4210], Loss: 0.4125, LR: 6.8e-06\n",
      "Epoch [4/10], Step [2601/4210], Loss: 0.5416, LR: 6.8e-06\n",
      "Epoch [4/10], Step [2801/4210], Loss: 0.6305, LR: 6.7e-06\n",
      "Epoch [4/10], Step [3001/4210], Loss: 0.4173, LR: 6.7e-06\n",
      "Epoch [4/10], Step [3201/4210], Loss: 0.4243, LR: 6.6e-06\n",
      "Epoch [4/10], Step [3401/4210], Loss: 0.3783, LR: 6.6e-06\n",
      "Epoch [4/10], Step [3601/4210], Loss: 0.5614, LR: 6.5e-06\n",
      "Epoch [4/10], Step [3801/4210], Loss: 0.4663, LR: 6.5e-06\n",
      "Epoch [4/10], Step [4001/4210], Loss: 0.3213, LR: 6.4e-06\n",
      "Epoch [4/10], Step [4201/4210], Loss: 0.5177, LR: 6.4e-06\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.1195\n",
      "    Validation loss: 0.4774\n",
      "    Accuracy: 0.7693\n",
      "========================================\n",
      "Epoch [5/10], Step [1/4210], Loss: 0.5641, LR: 6.4e-06\n",
      "Epoch [5/10], Step [201/4210], Loss: 0.4560, LR: 6.3e-06\n",
      "Epoch [5/10], Step [401/4210], Loss: 0.3178, LR: 6.3e-06\n",
      "Epoch [5/10], Step [601/4210], Loss: 0.5241, LR: 6.2e-06\n",
      "Epoch [5/10], Step [801/4210], Loss: 0.6437, LR: 6.2e-06\n",
      "Epoch [5/10], Step [1001/4210], Loss: 0.4798, LR: 6.1e-06\n",
      "Epoch [5/10], Step [1201/4210], Loss: 0.3633, LR: 6.1e-06\n",
      "Epoch [5/10], Step [1401/4210], Loss: 0.7529, LR: 6.0e-06\n",
      "Epoch [5/10], Step [1601/4210], Loss: 0.3881, LR: 6.0e-06\n",
      "Epoch [5/10], Step [1801/4210], Loss: 0.4417, LR: 5.9e-06\n",
      "Epoch [5/10], Step [2001/4210], Loss: 0.3999, LR: 5.9e-06\n",
      "Epoch [5/10], Step [2201/4210], Loss: 0.3830, LR: 5.8e-06\n",
      "Epoch [5/10], Step [2401/4210], Loss: 0.5058, LR: 5.8e-06\n",
      "Epoch [5/10], Step [2601/4210], Loss: 0.4669, LR: 5.7e-06\n",
      "Epoch [5/10], Step [2801/4210], Loss: 0.3771, LR: 5.7e-06\n",
      "Epoch [5/10], Step [3001/4210], Loss: 0.5938, LR: 5.6e-06\n",
      "Epoch [5/10], Step [3201/4210], Loss: 0.3557, LR: 5.6e-06\n",
      "Epoch [5/10], Step [3401/4210], Loss: 0.5161, LR: 5.5e-06\n",
      "Epoch [5/10], Step [3601/4210], Loss: 0.5416, LR: 5.5e-06\n",
      "Epoch [5/10], Step [3801/4210], Loss: 0.4841, LR: 5.4e-06\n",
      "Epoch [5/10], Step [4001/4210], Loss: 0.5057, LR: 5.4e-06\n",
      "Epoch [5/10], Step [4201/4210], Loss: 0.4136, LR: 5.3e-06\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.5736\n",
      "    Validation loss: 0.4793\n",
      "    Accuracy: 0.7557\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.8057 at epoch 1\n",
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 2.0e-05, batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/4210], Loss: 0.6943, LR: 1.0e-06\n",
      "Epoch [1/10], Step [201/4210], Loss: 0.6993, LR: 2.5e-06\n",
      "Epoch [1/10], Step [401/4210], Loss: 0.6640, LR: 4.0e-06\n",
      "Epoch [1/10], Step [601/4210], Loss: 0.6952, LR: 5.5e-06\n",
      "Epoch [1/10], Step [801/4210], Loss: 0.7208, LR: 7.0e-06\n",
      "Epoch [1/10], Step [1001/4210], Loss: 0.6912, LR: 8.5e-06\n",
      "Epoch [1/10], Step [1201/4210], Loss: 0.4687, LR: 1.0e-05\n",
      "Epoch [1/10], Step [1401/4210], Loss: 0.3762, LR: 1.2e-05\n",
      "Epoch [1/10], Step [1601/4210], Loss: 0.3398, LR: 1.3e-05\n",
      "Epoch [1/10], Step [1801/4210], Loss: 0.3931, LR: 1.5e-05\n",
      "Epoch [1/10], Step [2001/4210], Loss: 0.2426, LR: 1.6e-05\n",
      "Epoch [1/10], Step [2201/4210], Loss: 0.3989, LR: 1.8e-05\n",
      "Epoch [1/10], Step [2401/4210], Loss: 0.2200, LR: 1.9e-05\n",
      "Epoch [1/10], Step [2601/4210], Loss: 0.3853, LR: 2.0e-05\n",
      "Epoch [1/10], Step [2801/4210], Loss: 0.3133, LR: 2.0e-05\n",
      "Epoch [1/10], Step [3001/4210], Loss: 0.4344, LR: 2.0e-05\n",
      "Epoch [1/10], Step [3201/4210], Loss: 0.4969, LR: 2.0e-05\n",
      "Epoch [1/10], Step [3401/4210], Loss: 0.4348, LR: 2.0e-05\n",
      "Epoch [1/10], Step [3601/4210], Loss: 0.8082, LR: 1.9e-05\n",
      "Epoch [1/10], Step [3801/4210], Loss: 0.4492, LR: 1.9e-05\n",
      "Epoch [1/10], Step [4001/4210], Loss: 0.5501, LR: 1.9e-05\n",
      "Epoch [1/10], Step [4201/4210], Loss: 0.3702, LR: 1.9e-05\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.3553\n",
      "    Validation loss: 0.4909\n",
      "    Accuracy: 0.7420\n",
      "========================================\n",
      "Epoch [2/10], Step [1/4210], Loss: 0.8968, LR: 1.9e-05\n",
      "Epoch [2/10], Step [201/4210], Loss: 0.4074, LR: 1.9e-05\n",
      "Epoch [2/10], Step [401/4210], Loss: 0.5342, LR: 1.9e-05\n",
      "Epoch [2/10], Step [601/4210], Loss: 0.5599, LR: 1.9e-05\n",
      "Epoch [2/10], Step [801/4210], Loss: 0.4365, LR: 1.9e-05\n",
      "Epoch [2/10], Step [1001/4210], Loss: 0.6117, LR: 1.9e-05\n",
      "Epoch [2/10], Step [1201/4210], Loss: 0.4152, LR: 1.9e-05\n",
      "Epoch [2/10], Step [1401/4210], Loss: 0.4958, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1601/4210], Loss: 0.5491, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1801/4210], Loss: 0.5032, LR: 1.8e-05\n",
      "Epoch [2/10], Step [2001/4210], Loss: 0.6174, LR: 1.8e-05\n",
      "Epoch [2/10], Step [2201/4210], Loss: 0.4747, LR: 1.8e-05\n",
      "Epoch [2/10], Step [2401/4210], Loss: 0.4789, LR: 1.8e-05\n",
      "Epoch [2/10], Step [2601/4210], Loss: 0.5469, LR: 1.8e-05\n",
      "Epoch [2/10], Step [2801/4210], Loss: 0.5119, LR: 1.8e-05\n",
      "Epoch [2/10], Step [3001/4210], Loss: 0.5321, LR: 1.8e-05\n",
      "Epoch [2/10], Step [3201/4210], Loss: 0.5582, LR: 1.8e-05\n",
      "Epoch [2/10], Step [3401/4210], Loss: 0.5881, LR: 1.7e-05\n",
      "Epoch [2/10], Step [3601/4210], Loss: 0.4594, LR: 1.7e-05\n",
      "Epoch [2/10], Step [3801/4210], Loss: 0.5318, LR: 1.7e-05\n",
      "Epoch [2/10], Step [4001/4210], Loss: 0.5411, LR: 1.7e-05\n",
      "Epoch [2/10], Step [4201/4210], Loss: 0.4753, LR: 1.7e-05\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.7136\n",
      "    Validation loss: 0.4597\n",
      "    Accuracy: 0.7830\n",
      "========================================\n",
      "Epoch [3/10], Step [1/4210], Loss: 0.4373, LR: 1.7e-05\n",
      "Epoch [3/10], Step [201/4210], Loss: 0.5343, LR: 1.7e-05\n",
      "Epoch [3/10], Step [401/4210], Loss: 0.4987, LR: 1.7e-05\n",
      "Epoch [3/10], Step [601/4210], Loss: 0.4266, LR: 1.7e-05\n",
      "Epoch [3/10], Step [801/4210], Loss: 0.5636, LR: 1.7e-05\n",
      "Epoch [3/10], Step [1001/4210], Loss: 0.3228, LR: 1.7e-05\n",
      "Epoch [3/10], Step [1201/4210], Loss: 0.5531, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1401/4210], Loss: 0.4181, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1601/4210], Loss: 0.6068, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1801/4210], Loss: 0.7766, LR: 1.6e-05\n",
      "Epoch [3/10], Step [2001/4210], Loss: 0.6328, LR: 1.6e-05\n",
      "Epoch [3/10], Step [2201/4210], Loss: 0.4525, LR: 1.6e-05\n",
      "Epoch [3/10], Step [2401/4210], Loss: 0.5544, LR: 1.6e-05\n",
      "Epoch [3/10], Step [2601/4210], Loss: 0.6046, LR: 1.6e-05\n",
      "Epoch [3/10], Step [2801/4210], Loss: 0.5062, LR: 1.6e-05\n",
      "Epoch [3/10], Step [3001/4210], Loss: 0.5685, LR: 1.6e-05\n",
      "Epoch [3/10], Step [3201/4210], Loss: 0.5395, LR: 1.5e-05\n",
      "Epoch [3/10], Step [3401/4210], Loss: 0.5850, LR: 1.5e-05\n",
      "Epoch [3/10], Step [3601/4210], Loss: 0.6149, LR: 1.5e-05\n",
      "Epoch [3/10], Step [3801/4210], Loss: 0.7613, LR: 1.5e-05\n",
      "Epoch [3/10], Step [4001/4210], Loss: 0.5916, LR: 1.5e-05\n",
      "Epoch [3/10], Step [4201/4210], Loss: 0.5794, LR: 1.5e-05\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.3359\n",
      "    Validation loss: 0.5240\n",
      "    Accuracy: 0.7466\n",
      "========================================\n",
      "Epoch [4/10], Step [1/4210], Loss: 0.6191, LR: 1.5e-05\n",
      "Epoch [4/10], Step [201/4210], Loss: 0.7778, LR: 1.5e-05\n",
      "Epoch [4/10], Step [401/4210], Loss: 0.4203, LR: 1.5e-05\n",
      "Epoch [4/10], Step [601/4210], Loss: 0.5915, LR: 1.5e-05\n",
      "Epoch [4/10], Step [801/4210], Loss: 0.5788, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1001/4210], Loss: 0.7305, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1201/4210], Loss: 0.7180, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1401/4210], Loss: 0.5804, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1601/4210], Loss: 0.5637, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1801/4210], Loss: 0.5828, LR: 1.4e-05\n",
      "Epoch [4/10], Step [2001/4210], Loss: 0.6197, LR: 1.4e-05\n",
      "Epoch [4/10], Step [2201/4210], Loss: 0.5075, LR: 1.4e-05\n",
      "Epoch [4/10], Step [2401/4210], Loss: 0.5676, LR: 1.4e-05\n",
      "Epoch [4/10], Step [2601/4210], Loss: 0.7480, LR: 1.4e-05\n",
      "Epoch [4/10], Step [2801/4210], Loss: 0.8159, LR: 1.3e-05\n",
      "Epoch [4/10], Step [3001/4210], Loss: 0.6119, LR: 1.3e-05\n",
      "Epoch [4/10], Step [3201/4210], Loss: 0.5605, LR: 1.3e-05\n",
      "Epoch [4/10], Step [3401/4210], Loss: 0.6637, LR: 1.3e-05\n",
      "Epoch [4/10], Step [3601/4210], Loss: 0.7597, LR: 1.3e-05\n",
      "Epoch [4/10], Step [3801/4210], Loss: 0.6662, LR: 1.3e-05\n",
      "Epoch [4/10], Step [4001/4210], Loss: 0.6886, LR: 1.3e-05\n",
      "Epoch [4/10], Step [4201/4210], Loss: 0.4616, LR: 1.3e-05\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.5968\n",
      "    Validation loss: 0.5847\n",
      "    Accuracy: 0.7352\n",
      "========================================\n",
      "Epoch [5/10], Step [1/4210], Loss: 0.6090, LR: 1.3e-05\n",
      "Epoch [5/10], Step [201/4210], Loss: 0.7835, LR: 1.3e-05\n",
      "Epoch [5/10], Step [401/4210], Loss: 0.5638, LR: 1.3e-05\n",
      "Epoch [5/10], Step [601/4210], Loss: 0.6004, LR: 1.2e-05\n",
      "Epoch [5/10], Step [801/4210], Loss: 0.6290, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1001/4210], Loss: 0.5794, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1201/4210], Loss: 0.7083, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1401/4210], Loss: 0.6232, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1601/4210], Loss: 0.4705, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1801/4210], Loss: 0.6567, LR: 1.2e-05\n",
      "Epoch [5/10], Step [2001/4210], Loss: 0.6084, LR: 1.2e-05\n",
      "Epoch [5/10], Step [2201/4210], Loss: 0.5937, LR: 1.2e-05\n",
      "Epoch [5/10], Step [2401/4210], Loss: 0.6486, LR: 1.2e-05\n",
      "Epoch [5/10], Step [2601/4210], Loss: 0.6604, LR: 1.1e-05\n",
      "Epoch [5/10], Step [2801/4210], Loss: 0.6836, LR: 1.1e-05\n",
      "Epoch [5/10], Step [3001/4210], Loss: 0.6397, LR: 1.1e-05\n",
      "Epoch [5/10], Step [3201/4210], Loss: 0.6879, LR: 1.1e-05\n",
      "Epoch [5/10], Step [3401/4210], Loss: 0.6617, LR: 1.1e-05\n",
      "Epoch [5/10], Step [3601/4210], Loss: 0.6790, LR: 1.1e-05\n",
      "Epoch [5/10], Step [3801/4210], Loss: 0.7407, LR: 1.1e-05\n",
      "Epoch [5/10], Step [4001/4210], Loss: 0.6377, LR: 1.1e-05\n",
      "Epoch [5/10], Step [4201/4210], Loss: 0.6763, LR: 1.1e-05\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.6147\n",
      "    Validation loss: 0.6971\n",
      "    Accuracy: 0.5091\n",
      "========================================\n",
      "Epoch [6/10], Step [1/4210], Loss: 0.7009, LR: 1.1e-05\n",
      "Epoch [6/10], Step [201/4210], Loss: 0.6763, LR: 1.1e-05\n",
      "Epoch [6/10], Step [401/4210], Loss: 0.7020, LR: 1.0e-05\n",
      "Epoch [6/10], Step [601/4210], Loss: 0.6550, LR: 1.0e-05\n",
      "Epoch [6/10], Step [801/4210], Loss: 0.6488, LR: 1.0e-05\n",
      "Epoch [6/10], Step [1001/4210], Loss: 0.6938, LR: 1.0e-05\n",
      "Epoch [6/10], Step [1201/4210], Loss: 0.7222, LR: 1.0e-05\n",
      "Epoch [6/10], Step [1401/4210], Loss: 0.7037, LR: 9.9e-06\n",
      "Epoch [6/10], Step [1601/4210], Loss: 0.6984, LR: 9.8e-06\n",
      "Epoch [6/10], Step [1801/4210], Loss: 0.7058, LR: 9.7e-06\n",
      "Epoch [6/10], Step [2001/4210], Loss: 0.6525, LR: 9.6e-06\n",
      "Epoch [6/10], Step [2201/4210], Loss: 0.6670, LR: 9.5e-06\n",
      "Epoch [6/10], Step [2401/4210], Loss: 0.6733, LR: 9.4e-06\n",
      "Epoch [6/10], Step [2601/4210], Loss: 0.6842, LR: 9.3e-06\n",
      "Epoch [6/10], Step [2801/4210], Loss: 0.6908, LR: 9.2e-06\n",
      "Epoch [6/10], Step [3001/4210], Loss: 0.6350, LR: 9.1e-06\n",
      "Epoch [6/10], Step [3201/4210], Loss: 0.6653, LR: 9.0e-06\n",
      "Epoch [6/10], Step [3401/4210], Loss: 0.6520, LR: 8.9e-06\n",
      "Epoch [6/10], Step [3601/4210], Loss: 0.6684, LR: 8.8e-06\n",
      "Epoch [6/10], Step [3801/4210], Loss: 0.6575, LR: 8.7e-06\n",
      "Epoch [6/10], Step [4001/4210], Loss: 0.6741, LR: 8.6e-06\n",
      "Epoch [6/10], Step [4201/4210], Loss: 0.6906, LR: 8.5e-06\n",
      "Epoch [6/10] summary\n",
      "    Train loss: 0.6264\n",
      "    Validation loss: 0.6986\n",
      "    Accuracy: 0.5091\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.7830 at epoch 2\n",
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 3.0e-05, batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/4210], Loss: 0.6602, LR: 1.5e-06\n",
      "Epoch [1/10], Step [201/4210], Loss: 0.6812, LR: 3.8e-06\n",
      "Epoch [1/10], Step [401/4210], Loss: 0.6463, LR: 6.0e-06\n",
      "Epoch [1/10], Step [601/4210], Loss: 0.7134, LR: 8.3e-06\n",
      "Epoch [1/10], Step [801/4210], Loss: 0.5317, LR: 1.1e-05\n",
      "Epoch [1/10], Step [1001/4210], Loss: 0.4111, LR: 1.3e-05\n",
      "Epoch [1/10], Step [1201/4210], Loss: 0.5189, LR: 1.5e-05\n",
      "Epoch [1/10], Step [1401/4210], Loss: 0.4181, LR: 1.7e-05\n",
      "Epoch [1/10], Step [1601/4210], Loss: 0.1255, LR: 2.0e-05\n",
      "Epoch [1/10], Step [1801/4210], Loss: 0.3412, LR: 2.2e-05\n",
      "Epoch [1/10], Step [2001/4210], Loss: 0.2416, LR: 2.4e-05\n",
      "Epoch [1/10], Step [2201/4210], Loss: 0.4162, LR: 2.6e-05\n",
      "Epoch [1/10], Step [2401/4210], Loss: 0.5384, LR: 2.9e-05\n",
      "Epoch [1/10], Step [2601/4210], Loss: 0.3065, LR: 3.0e-05\n",
      "Epoch [1/10], Step [2801/4210], Loss: 0.3663, LR: 3.0e-05\n",
      "Epoch [1/10], Step [3001/4210], Loss: 0.4358, LR: 3.0e-05\n",
      "Epoch [1/10], Step [3201/4210], Loss: 0.4822, LR: 2.9e-05\n",
      "Epoch [1/10], Step [3401/4210], Loss: 0.3940, LR: 2.9e-05\n",
      "Epoch [1/10], Step [3601/4210], Loss: 0.6293, LR: 2.9e-05\n",
      "Epoch [1/10], Step [3801/4210], Loss: 0.5390, LR: 2.9e-05\n",
      "Epoch [1/10], Step [4001/4210], Loss: 0.6397, LR: 2.9e-05\n",
      "Epoch [1/10], Step [4201/4210], Loss: 0.3736, LR: 2.9e-05\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.4448\n",
      "    Validation loss: 0.4641\n",
      "    Accuracy: 0.7784\n",
      "========================================\n",
      "Epoch [2/10], Step [1/4210], Loss: 0.5427, LR: 2.9e-05\n",
      "Epoch [2/10], Step [201/4210], Loss: 0.4288, LR: 2.9e-05\n",
      "Epoch [2/10], Step [401/4210], Loss: 0.4710, LR: 2.8e-05\n",
      "Epoch [2/10], Step [601/4210], Loss: 0.4279, LR: 2.8e-05\n",
      "Epoch [2/10], Step [801/4210], Loss: 0.5543, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1001/4210], Loss: 0.4468, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1201/4210], Loss: 0.4834, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1401/4210], Loss: 0.5261, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1601/4210], Loss: 0.3773, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1801/4210], Loss: 0.6753, LR: 2.7e-05\n",
      "Epoch [2/10], Step [2001/4210], Loss: 0.5371, LR: 2.7e-05\n",
      "Epoch [2/10], Step [2201/4210], Loss: 0.5261, LR: 2.7e-05\n",
      "Epoch [2/10], Step [2401/4210], Loss: 0.5514, LR: 2.7e-05\n",
      "Epoch [2/10], Step [2601/4210], Loss: 0.5149, LR: 2.7e-05\n",
      "Epoch [2/10], Step [2801/4210], Loss: 0.5048, LR: 2.7e-05\n",
      "Epoch [2/10], Step [3001/4210], Loss: 0.3792, LR: 2.6e-05\n",
      "Epoch [2/10], Step [3201/4210], Loss: 0.5051, LR: 2.6e-05\n",
      "Epoch [2/10], Step [3401/4210], Loss: 0.6472, LR: 2.6e-05\n",
      "Epoch [2/10], Step [3601/4210], Loss: 0.6498, LR: 2.6e-05\n",
      "Epoch [2/10], Step [3801/4210], Loss: 0.5068, LR: 2.6e-05\n",
      "Epoch [2/10], Step [4001/4210], Loss: 0.6210, LR: 2.6e-05\n",
      "Epoch [2/10], Step [4201/4210], Loss: 0.6250, LR: 2.6e-05\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.7273\n",
      "    Validation loss: 0.5795\n",
      "    Accuracy: 0.6625\n",
      "========================================\n",
      "Epoch [3/10], Step [1/4210], Loss: 0.7237, LR: 2.6e-05\n",
      "Epoch [3/10], Step [201/4210], Loss: 0.5821, LR: 2.5e-05\n",
      "Epoch [3/10], Step [401/4210], Loss: 0.5768, LR: 2.5e-05\n",
      "Epoch [3/10], Step [601/4210], Loss: 0.5488, LR: 2.5e-05\n",
      "Epoch [3/10], Step [801/4210], Loss: 0.4897, LR: 2.5e-05\n",
      "Epoch [3/10], Step [1001/4210], Loss: 0.6316, LR: 2.5e-05\n",
      "Epoch [3/10], Step [1201/4210], Loss: 0.6642, LR: 2.5e-05\n",
      "Epoch [3/10], Step [1401/4210], Loss: 0.6272, LR: 2.4e-05\n",
      "Epoch [3/10], Step [1601/4210], Loss: 0.5705, LR: 2.4e-05\n",
      "Epoch [3/10], Step [1801/4210], Loss: 0.6506, LR: 2.4e-05\n",
      "Epoch [3/10], Step [2001/4210], Loss: 0.5415, LR: 2.4e-05\n",
      "Epoch [3/10], Step [2201/4210], Loss: 0.7277, LR: 2.4e-05\n",
      "Epoch [3/10], Step [2401/4210], Loss: 0.6532, LR: 2.4e-05\n",
      "Epoch [3/10], Step [2601/4210], Loss: 0.6081, LR: 2.4e-05\n",
      "Epoch [3/10], Step [2801/4210], Loss: 0.6065, LR: 2.3e-05\n",
      "Epoch [3/10], Step [3001/4210], Loss: 0.6667, LR: 2.3e-05\n",
      "Epoch [3/10], Step [3201/4210], Loss: 0.7297, LR: 2.3e-05\n",
      "Epoch [3/10], Step [3401/4210], Loss: 0.5239, LR: 2.3e-05\n",
      "Epoch [3/10], Step [3601/4210], Loss: 0.6606, LR: 2.3e-05\n",
      "Epoch [3/10], Step [3801/4210], Loss: 0.6970, LR: 2.3e-05\n",
      "Epoch [3/10], Step [4001/4210], Loss: 0.7007, LR: 2.2e-05\n",
      "Epoch [3/10], Step [4201/4210], Loss: 0.7007, LR: 2.2e-05\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.7863\n",
      "    Validation loss: 0.7003\n",
      "    Accuracy: 0.5091\n",
      "========================================\n",
      "Epoch [4/10], Step [1/4210], Loss: 0.6759, LR: 2.2e-05\n",
      "Epoch [4/10], Step [201/4210], Loss: 0.6731, LR: 2.2e-05\n",
      "Epoch [4/10], Step [401/4210], Loss: 0.6801, LR: 2.2e-05\n",
      "Epoch [4/10], Step [601/4210], Loss: 0.6366, LR: 2.2e-05\n",
      "Epoch [4/10], Step [801/4210], Loss: 0.6981, LR: 2.2e-05\n",
      "Epoch [4/10], Step [1001/4210], Loss: 0.6720, LR: 2.2e-05\n",
      "Epoch [4/10], Step [1201/4210], Loss: 0.6601, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1401/4210], Loss: 0.7000, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1601/4210], Loss: 0.7246, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1801/4210], Loss: 0.7185, LR: 2.1e-05\n",
      "Epoch [4/10], Step [2001/4210], Loss: 0.6743, LR: 2.1e-05\n",
      "Epoch [4/10], Step [2201/4210], Loss: 0.6932, LR: 2.1e-05\n",
      "Epoch [4/10], Step [2401/4210], Loss: 0.6675, LR: 2.1e-05\n",
      "Epoch [4/10], Step [2601/4210], Loss: 0.7204, LR: 2.0e-05\n",
      "Epoch [4/10], Step [2801/4210], Loss: 0.7016, LR: 2.0e-05\n",
      "Epoch [4/10], Step [3001/4210], Loss: 0.7050, LR: 2.0e-05\n",
      "Epoch [4/10], Step [3201/4210], Loss: 0.7154, LR: 2.0e-05\n",
      "Epoch [4/10], Step [3401/4210], Loss: 0.6782, LR: 2.0e-05\n",
      "Epoch [4/10], Step [3601/4210], Loss: 0.6990, LR: 2.0e-05\n",
      "Epoch [4/10], Step [3801/4210], Loss: 0.6842, LR: 1.9e-05\n",
      "Epoch [4/10], Step [4001/4210], Loss: 0.6644, LR: 1.9e-05\n",
      "Epoch [4/10], Step [4201/4210], Loss: 0.7171, LR: 1.9e-05\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.7245\n",
      "    Validation loss: 0.6982\n",
      "    Accuracy: 0.5091\n",
      "========================================\n",
      "Epoch [5/10], Step [1/4210], Loss: 0.7139, LR: 1.9e-05\n",
      "Epoch [5/10], Step [201/4210], Loss: 0.6635, LR: 1.9e-05\n",
      "Epoch [5/10], Step [401/4210], Loss: 0.6889, LR: 1.9e-05\n",
      "Epoch [5/10], Step [601/4210], Loss: 0.6828, LR: 1.9e-05\n",
      "Epoch [5/10], Step [801/4210], Loss: 0.6436, LR: 1.9e-05\n",
      "Epoch [5/10], Step [1001/4210], Loss: 0.7026, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1201/4210], Loss: 0.6530, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1401/4210], Loss: 0.6902, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1601/4210], Loss: 0.6227, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1801/4210], Loss: 0.7129, LR: 1.8e-05\n",
      "Epoch [5/10], Step [2001/4210], Loss: 0.6745, LR: 1.8e-05\n",
      "Epoch [5/10], Step [2201/4210], Loss: 0.6741, LR: 1.7e-05\n",
      "Epoch [5/10], Step [2401/4210], Loss: 0.7011, LR: 1.7e-05\n",
      "Epoch [5/10], Step [2601/4210], Loss: 0.6977, LR: 1.7e-05\n",
      "Epoch [5/10], Step [2801/4210], Loss: 0.7552, LR: 1.7e-05\n",
      "Epoch [5/10], Step [3001/4210], Loss: 0.6999, LR: 1.7e-05\n",
      "Epoch [5/10], Step [3201/4210], Loss: 0.6176, LR: 1.7e-05\n",
      "Epoch [5/10], Step [3401/4210], Loss: 0.6786, LR: 1.7e-05\n",
      "Epoch [5/10], Step [3601/4210], Loss: 0.6600, LR: 1.6e-05\n",
      "Epoch [5/10], Step [3801/4210], Loss: 0.6959, LR: 1.6e-05\n",
      "Epoch [5/10], Step [4001/4210], Loss: 0.6869, LR: 1.6e-05\n",
      "Epoch [5/10], Step [4201/4210], Loss: 0.7204, LR: 1.6e-05\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.6779\n",
      "    Validation loss: 0.6965\n",
      "    Accuracy: 0.5091\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.7784 at epoch 1\n",
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 1.0e-05, batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/2105], Loss: 0.6905, LR: 5.1e-07\n",
      "Epoch [1/10], Step [201/2105], Loss: 0.6804, LR: 2.0e-06\n",
      "Epoch [1/10], Step [401/2105], Loss: 0.6536, LR: 3.5e-06\n",
      "Epoch [1/10], Step [601/2105], Loss: 0.6863, LR: 5.0e-06\n",
      "Epoch [1/10], Step [801/2105], Loss: 0.6745, LR: 6.5e-06\n",
      "Epoch [1/10], Step [1001/2105], Loss: 0.6039, LR: 8.0e-06\n",
      "Epoch [1/10], Step [1201/2105], Loss: 0.5036, LR: 9.5e-06\n",
      "Epoch [1/10], Step [1401/2105], Loss: 0.3109, LR: 9.9e-06\n",
      "Epoch [1/10], Step [1601/2105], Loss: 0.2093, LR: 9.8e-06\n",
      "Epoch [1/10], Step [1801/2105], Loss: 0.4837, LR: 9.7e-06\n",
      "Epoch [1/10], Step [2001/2105], Loss: 0.4262, LR: 9.6e-06\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.2878\n",
      "    Validation loss: 0.2704\n",
      "    Accuracy: 0.9007\n",
      "========================================\n",
      "Epoch [2/10], Step [1/2105], Loss: 0.2898, LR: 9.6e-06\n",
      "Epoch [2/10], Step [201/2105], Loss: 0.4114, LR: 9.5e-06\n",
      "Epoch [2/10], Step [401/2105], Loss: 0.3102, LR: 9.4e-06\n",
      "Epoch [2/10], Step [601/2105], Loss: 0.3495, LR: 9.3e-06\n",
      "Epoch [2/10], Step [801/2105], Loss: 0.2725, LR: 9.2e-06\n",
      "Epoch [2/10], Step [1001/2105], Loss: 0.3503, LR: 9.1e-06\n",
      "Epoch [2/10], Step [1201/2105], Loss: 0.5455, LR: 9.0e-06\n",
      "Epoch [2/10], Step [1401/2105], Loss: 0.3323, LR: 8.9e-06\n",
      "Epoch [2/10], Step [1601/2105], Loss: 0.2118, LR: 8.8e-06\n",
      "Epoch [2/10], Step [1801/2105], Loss: 0.3829, LR: 8.7e-06\n",
      "Epoch [2/10], Step [2001/2105], Loss: 0.4179, LR: 8.6e-06\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.4756\n",
      "    Validation loss: 0.4056\n",
      "    Accuracy: 0.8192\n",
      "========================================\n",
      "Epoch [3/10], Step [1/2105], Loss: 0.4530, LR: 8.5e-06\n",
      "Epoch [3/10], Step [201/2105], Loss: 0.3874, LR: 8.4e-06\n",
      "Epoch [3/10], Step [401/2105], Loss: 0.3628, LR: 8.3e-06\n",
      "Epoch [3/10], Step [601/2105], Loss: 0.2940, LR: 8.2e-06\n",
      "Epoch [3/10], Step [801/2105], Loss: 0.4135, LR: 8.1e-06\n",
      "Epoch [3/10], Step [1001/2105], Loss: 0.3169, LR: 8.0e-06\n",
      "Epoch [3/10], Step [1201/2105], Loss: 0.4543, LR: 7.9e-06\n",
      "Epoch [3/10], Step [1401/2105], Loss: 0.3968, LR: 7.8e-06\n",
      "Epoch [3/10], Step [1601/2105], Loss: 0.4119, LR: 7.7e-06\n",
      "Epoch [3/10], Step [1801/2105], Loss: 0.5530, LR: 7.6e-06\n",
      "Epoch [3/10], Step [2001/2105], Loss: 0.4977, LR: 7.5e-06\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.3407\n",
      "    Validation loss: 0.4252\n",
      "    Accuracy: 0.8080\n",
      "========================================\n",
      "Epoch [4/10], Step [1/2105], Loss: 0.3828, LR: 7.4e-06\n",
      "Epoch [4/10], Step [201/2105], Loss: 0.3556, LR: 7.3e-06\n",
      "Epoch [4/10], Step [401/2105], Loss: 0.4470, LR: 7.2e-06\n",
      "Epoch [4/10], Step [601/2105], Loss: 0.5488, LR: 7.1e-06\n",
      "Epoch [4/10], Step [801/2105], Loss: 0.5835, LR: 7.0e-06\n",
      "Epoch [4/10], Step [1001/2105], Loss: 0.4717, LR: 6.9e-06\n",
      "Epoch [4/10], Step [1201/2105], Loss: 0.3150, LR: 6.8e-06\n",
      "Epoch [4/10], Step [1401/2105], Loss: 0.3940, LR: 6.7e-06\n",
      "Epoch [4/10], Step [1601/2105], Loss: 0.7372, LR: 6.6e-06\n",
      "Epoch [4/10], Step [1801/2105], Loss: 0.5382, LR: 6.5e-06\n",
      "Epoch [4/10], Step [2001/2105], Loss: 0.3251, LR: 6.4e-06\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.4352\n",
      "    Validation loss: 0.5127\n",
      "    Accuracy: 0.7556\n",
      "========================================\n",
      "Epoch [5/10], Step [1/2105], Loss: 0.5492, LR: 6.4e-06\n",
      "Epoch [5/10], Step [201/2105], Loss: 0.4528, LR: 6.3e-06\n",
      "Epoch [5/10], Step [401/2105], Loss: 0.4512, LR: 6.2e-06\n",
      "Epoch [5/10], Step [601/2105], Loss: 0.4349, LR: 6.1e-06\n",
      "Epoch [5/10], Step [801/2105], Loss: 0.4087, LR: 6.0e-06\n",
      "Epoch [5/10], Step [1001/2105], Loss: 0.5288, LR: 5.9e-06\n",
      "Epoch [5/10], Step [1201/2105], Loss: 0.3719, LR: 5.8e-06\n",
      "Epoch [5/10], Step [1401/2105], Loss: 0.3499, LR: 5.7e-06\n",
      "Epoch [5/10], Step [1601/2105], Loss: 0.5455, LR: 5.6e-06\n",
      "Epoch [5/10], Step [1801/2105], Loss: 0.3925, LR: 5.5e-06\n",
      "Epoch [5/10], Step [2001/2105], Loss: 0.5189, LR: 5.4e-06\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.2016\n",
      "    Validation loss: 0.4347\n",
      "    Accuracy: 0.7801\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.9007 at epoch 1\n",
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 2.0e-05, batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/2105], Loss: 0.6873, LR: 1.0e-06\n",
      "Epoch [1/10], Step [201/2105], Loss: 0.6931, LR: 4.0e-06\n",
      "Epoch [1/10], Step [401/2105], Loss: 0.6867, LR: 7.0e-06\n",
      "Epoch [1/10], Step [601/2105], Loss: 0.4485, LR: 1.0e-05\n",
      "Epoch [1/10], Step [801/2105], Loss: 0.2590, LR: 1.3e-05\n",
      "Epoch [1/10], Step [1001/2105], Loss: 0.2670, LR: 1.6e-05\n",
      "Epoch [1/10], Step [1201/2105], Loss: 0.2897, LR: 1.9e-05\n",
      "Epoch [1/10], Step [1401/2105], Loss: 0.2530, LR: 2.0e-05\n",
      "Epoch [1/10], Step [1601/2105], Loss: 0.3157, LR: 2.0e-05\n",
      "Epoch [1/10], Step [1801/2105], Loss: 0.3355, LR: 1.9e-05\n",
      "Epoch [1/10], Step [2001/2105], Loss: 0.3442, LR: 1.9e-05\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.2729\n",
      "    Validation loss: 0.3929\n",
      "    Accuracy: 0.8237\n",
      "========================================\n",
      "Epoch [2/10], Step [1/2105], Loss: 0.5464, LR: 1.9e-05\n",
      "Epoch [2/10], Step [201/2105], Loss: 0.3945, LR: 1.9e-05\n",
      "Epoch [2/10], Step [401/2105], Loss: 0.4191, LR: 1.9e-05\n",
      "Epoch [2/10], Step [601/2105], Loss: 0.5094, LR: 1.9e-05\n",
      "Epoch [2/10], Step [801/2105], Loss: 0.3591, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1001/2105], Loss: 0.4472, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1201/2105], Loss: 0.4143, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1401/2105], Loss: 0.2969, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1601/2105], Loss: 0.5698, LR: 1.8e-05\n",
      "Epoch [2/10], Step [1801/2105], Loss: 0.4877, LR: 1.7e-05\n",
      "Epoch [2/10], Step [2001/2105], Loss: 0.3038, LR: 1.7e-05\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.3388\n",
      "    Validation loss: 0.4368\n",
      "    Accuracy: 0.7779\n",
      "========================================\n",
      "Epoch [3/10], Step [1/2105], Loss: 0.4215, LR: 1.7e-05\n",
      "Epoch [3/10], Step [201/2105], Loss: 0.5283, LR: 1.7e-05\n",
      "Epoch [3/10], Step [401/2105], Loss: 0.4183, LR: 1.7e-05\n",
      "Epoch [3/10], Step [601/2105], Loss: 0.4875, LR: 1.6e-05\n",
      "Epoch [3/10], Step [801/2105], Loss: 0.3764, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1001/2105], Loss: 0.3738, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1201/2105], Loss: 0.4482, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1401/2105], Loss: 0.5838, LR: 1.6e-05\n",
      "Epoch [3/10], Step [1601/2105], Loss: 0.3470, LR: 1.5e-05\n",
      "Epoch [3/10], Step [1801/2105], Loss: 0.3599, LR: 1.5e-05\n",
      "Epoch [3/10], Step [2001/2105], Loss: 0.4340, LR: 1.5e-05\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.7773\n",
      "    Validation loss: 0.4311\n",
      "    Accuracy: 0.7913\n",
      "========================================\n",
      "Epoch [4/10], Step [1/2105], Loss: 0.5129, LR: 1.5e-05\n",
      "Epoch [4/10], Step [201/2105], Loss: 0.3778, LR: 1.5e-05\n",
      "Epoch [4/10], Step [401/2105], Loss: 0.3132, LR: 1.4e-05\n",
      "Epoch [4/10], Step [601/2105], Loss: 0.5229, LR: 1.4e-05\n",
      "Epoch [4/10], Step [801/2105], Loss: 0.4648, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1001/2105], Loss: 0.5322, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1201/2105], Loss: 0.4068, LR: 1.4e-05\n",
      "Epoch [4/10], Step [1401/2105], Loss: 0.4374, LR: 1.3e-05\n",
      "Epoch [4/10], Step [1601/2105], Loss: 0.5666, LR: 1.3e-05\n",
      "Epoch [4/10], Step [1801/2105], Loss: 0.5141, LR: 1.3e-05\n",
      "Epoch [4/10], Step [2001/2105], Loss: 0.4324, LR: 1.3e-05\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.5932\n",
      "    Validation loss: 0.4437\n",
      "    Accuracy: 0.7835\n",
      "========================================\n",
      "Epoch [5/10], Step [1/2105], Loss: 0.3906, LR: 1.3e-05\n",
      "Epoch [5/10], Step [201/2105], Loss: 0.4415, LR: 1.3e-05\n",
      "Epoch [5/10], Step [401/2105], Loss: 0.2491, LR: 1.2e-05\n",
      "Epoch [5/10], Step [601/2105], Loss: 0.4769, LR: 1.2e-05\n",
      "Epoch [5/10], Step [801/2105], Loss: 0.4476, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1001/2105], Loss: 0.5258, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1201/2105], Loss: 0.6600, LR: 1.2e-05\n",
      "Epoch [5/10], Step [1401/2105], Loss: 0.5675, LR: 1.1e-05\n",
      "Epoch [5/10], Step [1601/2105], Loss: 0.3451, LR: 1.1e-05\n",
      "Epoch [5/10], Step [1801/2105], Loss: 0.5006, LR: 1.1e-05\n",
      "Epoch [5/10], Step [2001/2105], Loss: 0.5117, LR: 1.1e-05\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.2987\n",
      "    Validation loss: 0.4410\n",
      "    Accuracy: 0.7980\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.8237 at epoch 1\n",
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 3.0e-05, batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/2105], Loss: 0.6871, LR: 1.5e-06\n",
      "Epoch [1/10], Step [201/2105], Loss: 0.7208, LR: 6.0e-06\n",
      "Epoch [1/10], Step [401/2105], Loss: 0.7037, LR: 1.1e-05\n",
      "Epoch [1/10], Step [601/2105], Loss: 0.4366, LR: 1.5e-05\n",
      "Epoch [1/10], Step [801/2105], Loss: 0.5549, LR: 2.0e-05\n",
      "Epoch [1/10], Step [1001/2105], Loss: 0.2791, LR: 2.4e-05\n",
      "Epoch [1/10], Step [1201/2105], Loss: 0.3378, LR: 2.9e-05\n",
      "Epoch [1/10], Step [1401/2105], Loss: 0.4560, LR: 3.0e-05\n",
      "Epoch [1/10], Step [1601/2105], Loss: 0.4812, LR: 2.9e-05\n",
      "Epoch [1/10], Step [1801/2105], Loss: 0.3306, LR: 2.9e-05\n",
      "Epoch [1/10], Step [2001/2105], Loss: 0.4417, LR: 2.9e-05\n",
      "Epoch [1/10] summary\n",
      "    Train loss: 0.3730\n",
      "    Validation loss: 0.4793\n",
      "    Accuracy: 0.7690\n",
      "========================================\n",
      "Epoch [2/10], Step [1/2105], Loss: 0.4485, LR: 2.9e-05\n",
      "Epoch [2/10], Step [201/2105], Loss: 0.4830, LR: 2.8e-05\n",
      "Epoch [2/10], Step [401/2105], Loss: 0.3806, LR: 2.8e-05\n",
      "Epoch [2/10], Step [601/2105], Loss: 0.4889, LR: 2.8e-05\n",
      "Epoch [2/10], Step [801/2105], Loss: 0.6567, LR: 2.8e-05\n",
      "Epoch [2/10], Step [1001/2105], Loss: 0.5208, LR: 2.7e-05\n",
      "Epoch [2/10], Step [1201/2105], Loss: 0.4981, LR: 2.7e-05\n",
      "Epoch [2/10], Step [1401/2105], Loss: 0.5260, LR: 2.7e-05\n",
      "Epoch [2/10], Step [1601/2105], Loss: 0.3936, LR: 2.6e-05\n",
      "Epoch [2/10], Step [1801/2105], Loss: 0.4027, LR: 2.6e-05\n",
      "Epoch [2/10], Step [2001/2105], Loss: 0.3499, LR: 2.6e-05\n",
      "Epoch [2/10] summary\n",
      "    Train loss: 0.4836\n",
      "    Validation loss: 0.4530\n",
      "    Accuracy: 0.7946\n",
      "========================================\n",
      "Epoch [3/10], Step [1/2105], Loss: 0.4266, LR: 2.6e-05\n",
      "Epoch [3/10], Step [201/2105], Loss: 0.3613, LR: 2.5e-05\n",
      "Epoch [3/10], Step [401/2105], Loss: 0.6265, LR: 2.5e-05\n",
      "Epoch [3/10], Step [601/2105], Loss: 0.4497, LR: 2.5e-05\n",
      "Epoch [3/10], Step [801/2105], Loss: 0.5052, LR: 2.4e-05\n",
      "Epoch [3/10], Step [1001/2105], Loss: 0.5964, LR: 2.4e-05\n",
      "Epoch [3/10], Step [1201/2105], Loss: 0.4430, LR: 2.4e-05\n",
      "Epoch [3/10], Step [1401/2105], Loss: 0.5318, LR: 2.3e-05\n",
      "Epoch [3/10], Step [1601/2105], Loss: 0.5668, LR: 2.3e-05\n",
      "Epoch [3/10], Step [1801/2105], Loss: 0.5992, LR: 2.3e-05\n",
      "Epoch [3/10], Step [2001/2105], Loss: 0.7387, LR: 2.2e-05\n",
      "Epoch [3/10] summary\n",
      "    Train loss: 0.2773\n",
      "    Validation loss: 0.4563\n",
      "    Accuracy: 0.7913\n",
      "========================================\n",
      "Epoch [4/10], Step [1/2105], Loss: 0.5137, LR: 2.2e-05\n",
      "Epoch [4/10], Step [201/2105], Loss: 0.4834, LR: 2.2e-05\n",
      "Epoch [4/10], Step [401/2105], Loss: 0.3538, LR: 2.2e-05\n",
      "Epoch [4/10], Step [601/2105], Loss: 0.6408, LR: 2.1e-05\n",
      "Epoch [4/10], Step [801/2105], Loss: 0.4303, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1001/2105], Loss: 0.5969, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1201/2105], Loss: 0.5874, LR: 2.1e-05\n",
      "Epoch [4/10], Step [1401/2105], Loss: 0.5301, LR: 2.0e-05\n",
      "Epoch [4/10], Step [1601/2105], Loss: 0.4952, LR: 2.0e-05\n",
      "Epoch [4/10], Step [1801/2105], Loss: 0.7333, LR: 2.0e-05\n",
      "Epoch [4/10], Step [2001/2105], Loss: 0.4958, LR: 1.9e-05\n",
      "Epoch [4/10] summary\n",
      "    Train loss: 0.4182\n",
      "    Validation loss: 0.4919\n",
      "    Accuracy: 0.7734\n",
      "========================================\n",
      "Epoch [5/10], Step [1/2105], Loss: 0.4674, LR: 1.9e-05\n",
      "Epoch [5/10], Step [201/2105], Loss: 0.4763, LR: 1.9e-05\n",
      "Epoch [5/10], Step [401/2105], Loss: 0.6652, LR: 1.9e-05\n",
      "Epoch [5/10], Step [601/2105], Loss: 0.5632, LR: 1.8e-05\n",
      "Epoch [5/10], Step [801/2105], Loss: 0.6364, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1001/2105], Loss: 0.4930, LR: 1.8e-05\n",
      "Epoch [5/10], Step [1201/2105], Loss: 0.5273, LR: 1.7e-05\n",
      "Epoch [5/10], Step [1401/2105], Loss: 0.5383, LR: 1.7e-05\n",
      "Epoch [5/10], Step [1601/2105], Loss: 0.5563, LR: 1.7e-05\n",
      "Epoch [5/10], Step [1801/2105], Loss: 0.6408, LR: 1.6e-05\n",
      "Epoch [5/10], Step [2001/2105], Loss: 0.5375, LR: 1.6e-05\n",
      "Epoch [5/10] summary\n",
      "    Train loss: 0.4544\n",
      "    Validation loss: 0.4976\n",
      "    Accuracy: 0.7812\n",
      "========================================\n",
      "Epoch [6/10], Step [1/2105], Loss: 0.5788, LR: 1.6e-05\n",
      "Epoch [6/10], Step [201/2105], Loss: 0.5052, LR: 1.6e-05\n",
      "Epoch [6/10], Step [401/2105], Loss: 0.4934, LR: 1.5e-05\n",
      "Epoch [6/10], Step [601/2105], Loss: 0.5288, LR: 1.5e-05\n",
      "Epoch [6/10], Step [801/2105], Loss: 0.6200, LR: 1.5e-05\n",
      "Epoch [6/10], Step [1001/2105], Loss: 0.6238, LR: 1.4e-05\n",
      "Epoch [6/10], Step [1201/2105], Loss: 0.6895, LR: 1.4e-05\n",
      "Epoch [6/10], Step [1401/2105], Loss: 0.5872, LR: 1.4e-05\n",
      "Epoch [6/10], Step [1601/2105], Loss: 0.5709, LR: 1.4e-05\n",
      "Epoch [6/10], Step [1801/2105], Loss: 0.7031, LR: 1.3e-05\n",
      "Epoch [6/10], Step [2001/2105], Loss: 0.6640, LR: 1.3e-05\n",
      "Epoch [6/10] summary\n",
      "    Train loss: 0.3645\n",
      "    Validation loss: 0.5239\n",
      "    Accuracy: 0.7310\n",
      "========================================\n",
      "-------- Early stopping --------------\n",
      "Best accuracy is 0.7946 at epoch 2\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "all_runs = []\n",
    "for batch_size in [16, 32]:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    warmup_ratio = 0.06\n",
    "    warmup_steps = warmup_ratio * total_steps\n",
    "    patience = 3 # early stopping after 3 epochs with no improvement\n",
    "\n",
    "    for learning_rate in [1e-5, 2e-5, 3e-5]:\n",
    "        print('========= RUN PARAMETERS: ===============')\n",
    "        print(f'Learning rate: {learning_rate:.1e}, batch size: {batch_size}')\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = Adam(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.98),\n",
    "            weight_decay=0.1,\n",
    "            eps=1e-6\n",
    "        )\n",
    "\n",
    "        warmup_scheduler = LinearLR(\n",
    "            optimizer=optimizer,\n",
    "            start_factor=0.05,\n",
    "            end_factor=1.0,\n",
    "            total_iters=warmup_steps,\n",
    "        )\n",
    "        decay_scheduler = LinearLR(\n",
    "            optimizer=optimizer,\n",
    "            start_factor=1.0,\n",
    "            end_factor=0.0,\n",
    "            total_iters=total_steps - warmup_steps,\n",
    "        )\n",
    "\n",
    "        scheduler = SequentialLR(\n",
    "            optimizer=optimizer,\n",
    "            schedulers=[warmup_scheduler, decay_scheduler],\n",
    "            milestones=[warmup_steps]\n",
    "        )\n",
    "\n",
    "        min_val_loss = float('inf')\n",
    "        epoch_since_last_loss_improvement = 0\n",
    "\n",
    "        # training loop\n",
    "        train_losses, val_losses, accuracies = [], [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "                # forward\n",
    "                logits = model(**x).logits\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "                if batch_idx % 200 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "            \n",
    "            # get train loss\n",
    "            train_loss, _ = get_loss_and_accuracy(\n",
    "                model=model,\n",
    "                dataset=train_dataset,\n",
    "                device=device,\n",
    "                eval_ratio=0.1 # evaluate on 10% of train data\n",
    "            )\n",
    "\n",
    "            # Get validation loss and accuracy\n",
    "            val_loss, val_accuracy = get_loss_and_accuracy(\n",
    "                model=model,\n",
    "                dataset=val_dataset,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] summary\")\n",
    "            print(f'    Train loss: {train_loss:.4f}')\n",
    "            print(f'    Validation loss: {val_loss:.4f}')\n",
    "            print(f'    Accuracy: {val_accuracy:.4f}')\n",
    "            print('========================================')\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                epoch_since_last_loss_improvement = 0\n",
    "            else:\n",
    "                epoch_since_last_loss_improvement += 1\n",
    "                if epoch_since_last_loss_improvement > patience:\n",
    "                    print(\"-------- Early stopping --------------\")\n",
    "                    break\n",
    "\n",
    "        all_runs.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'accuracies': accuracies,\n",
    "        })\n",
    "\n",
    "        print(f'Best accuracy is {max(accuracies):.4f} at epoch {accuracies.index(max(accuracies)) + 1}')\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_runs.json', 'w') as f:\n",
    "    f.write(str(all_runs).replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_losses</th>\n",
       "      <th>val_losses</th>\n",
       "      <th>accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.12, 0.4, 0.15, 0.12, 0.57]</td>\n",
       "      <td>[0.41, 0.44, 0.46, 0.48, 0.48]</td>\n",
       "      <td>[0.81, 0.8, 0.78, 0.77, 0.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.36, 0.71, 0.34, 0.6, 0.61, 0.63]</td>\n",
       "      <td>[0.49, 0.46, 0.52, 0.58, 0.7, 0.7]</td>\n",
       "      <td>[0.74, 0.78, 0.75, 0.74, 0.51, 0.51]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.44, 0.73, 0.79, 0.72, 0.68]</td>\n",
       "      <td>[0.46, 0.58, 0.7, 0.7, 0.7]</td>\n",
       "      <td>[0.78, 0.66, 0.51, 0.51, 0.51]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>[0.29, 0.48, 0.34, 0.44, 0.2]</td>\n",
       "      <td>[0.27, 0.41, 0.43, 0.51, 0.43]</td>\n",
       "      <td>[0.9, 0.82, 0.81, 0.76, 0.78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>[0.27, 0.34, 0.78, 0.59, 0.3]</td>\n",
       "      <td>[0.39, 0.44, 0.43, 0.44, 0.44]</td>\n",
       "      <td>[0.82, 0.78, 0.79, 0.78, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>[0.37, 0.48, 0.28, 0.42, 0.45, 0.36]</td>\n",
       "      <td>[0.48, 0.45, 0.46, 0.49, 0.5, 0.52]</td>\n",
       "      <td>[0.77, 0.79, 0.79, 0.77, 0.78, 0.73]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  batch_size                          train_losses  \\\n",
       "0            0.0          16         [0.12, 0.4, 0.15, 0.12, 0.57]   \n",
       "1            0.0          16   [0.36, 0.71, 0.34, 0.6, 0.61, 0.63]   \n",
       "2            0.0          16        [0.44, 0.73, 0.79, 0.72, 0.68]   \n",
       "3            0.0          32         [0.29, 0.48, 0.34, 0.44, 0.2]   \n",
       "4            0.0          32         [0.27, 0.34, 0.78, 0.59, 0.3]   \n",
       "5            0.0          32  [0.37, 0.48, 0.28, 0.42, 0.45, 0.36]   \n",
       "\n",
       "                            val_losses                            accuracies  \n",
       "0       [0.41, 0.44, 0.46, 0.48, 0.48]         [0.81, 0.8, 0.78, 0.77, 0.76]  \n",
       "1   [0.49, 0.46, 0.52, 0.58, 0.7, 0.7]  [0.74, 0.78, 0.75, 0.74, 0.51, 0.51]  \n",
       "2          [0.46, 0.58, 0.7, 0.7, 0.7]        [0.78, 0.66, 0.51, 0.51, 0.51]  \n",
       "3       [0.27, 0.41, 0.43, 0.51, 0.43]         [0.9, 0.82, 0.81, 0.76, 0.78]  \n",
       "4       [0.39, 0.44, 0.43, 0.44, 0.44]         [0.82, 0.78, 0.79, 0.78, 0.8]  \n",
       "5  [0.48, 0.45, 0.46, 0.49, 0.5, 0.52]  [0.77, 0.79, 0.79, 0.77, 0.78, 0.73]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_runs)\n",
    "df['train_losses'] = df['train_losses'].apply(lambda x: [round(i, 2) for i in x])\n",
    "df['val_losses'] = df['val_losses'].apply(lambda x: [round(i, 2) for i in x])\n",
    "df['accuracies'] = df['accuracies'].apply(lambda x: [round(i, 2) for i in x])\n",
    "\n",
    "df['learning_rate'] = df['learning_rate'].round(2)\n",
    "df['batch_size'] = df['batch_size'].astype(int)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/1053], Loss: 0.7357\n",
      "Epoch [1/2], Step [101/1053], Loss: 0.7078\n",
      "Epoch [1/2], Step [201/1053], Loss: 0.6649\n",
      "Epoch [1/2], Step [301/1053], Loss: 0.3061\n",
      "Epoch [1/2], Step [401/1053], Loss: 0.3070\n",
      "Epoch [1/2], Step [501/1053], Loss: 0.2225\n",
      "Epoch [1/2], Step [601/1053], Loss: 0.1832\n",
      "Epoch [1/2], Step [701/1053], Loss: 0.1393\n",
      "Epoch [1/2], Step [801/1053], Loss: 0.2323\n",
      "Epoch [1/2], Step [901/1053], Loss: 0.2204\n",
      "Epoch [1/2], Step [1001/1053], Loss: 0.2159\n",
      "Epoch [1/2] summary\n",
      "    Train loss: 0.2152\n",
      "    Validation loss: 0.1691\n",
      "    Accuracy: 0.9362\n",
      "========================================\n",
      "Epoch [2/2], Step [1/1053], Loss: 0.2890\n",
      "Epoch [2/2], Step [101/1053], Loss: 0.2561\n",
      "Epoch [2/2], Step [201/1053], Loss: 0.1747\n",
      "Epoch [2/2], Step [301/1053], Loss: 0.1304\n",
      "Epoch [2/2], Step [401/1053], Loss: 0.2262\n",
      "Epoch [2/2], Step [501/1053], Loss: 0.2732\n",
      "Epoch [2/2], Step [601/1053], Loss: 0.1889\n",
      "Epoch [2/2], Step [701/1053], Loss: 0.1037\n",
      "Epoch [2/2], Step [801/1053], Loss: 0.3003\n",
      "Epoch [2/2], Step [901/1053], Loss: 0.1361\n",
      "Epoch [2/2], Step [1001/1053], Loss: 0.3336\n",
      "Epoch [2/2] summary\n",
      "    Train loss: 0.0667\n",
      "    Validation loss: 0.1837\n",
      "    Accuracy: 0.9368\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "optimizer = Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=5e-6,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "        # forward\n",
    "        logits = model(**x).logits\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # get train loss\n",
    "    train_loss, _ = get_loss_and_accuracy(\n",
    "        model=model,\n",
    "        dataset=train_dataset,\n",
    "        device=device,\n",
    "        eval_ratio=0.1 # evaluate on 10% of train data\n",
    "    )\n",
    "\n",
    "    # Get validation loss and accuracy\n",
    "    val_loss, val_accuracy = get_loss_and_accuracy(\n",
    "        model=model,\n",
    "        dataset=val_dataset,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] summary\")\n",
    "    print(f'    Train loss: {train_loss:.4f}')\n",
    "    print(f'    Validation loss: {val_loss:.4f}')\n",
    "    print(f'    Accuracy: {val_accuracy:.4f}')\n",
    "    print('========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/best_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
