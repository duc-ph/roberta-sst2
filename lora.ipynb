{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning RoBERTa using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from lora_modules import LoRARobertaSdpaSelfAttention\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "from utils import get_loss_and_accuracy, SST2Dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm creating 2 identical instances of the RoBERTa-base model. We will keep one intact and modify the other with LoRA modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "torch.manual_seed(42)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "torch.manual_seed(42)\n",
    "model_original = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "model.to(device)\n",
    "model_original.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture:\n",
    "\n",
    "Before we start, let's take a look at the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data flow:\n",
    "\n",
    "* Firstly, the input tokens are converted into embeddings by the `RobertaEmbeddings` layer. \n",
    "\n",
    "* The embeddings are then fed into the `RobertaEncoder`, which consists of 12 stacked `RobertaLayers`. Each layer has a **self-attention mechanism**, along with feed-forward network.\n",
    "\n",
    "* The encoder outputs go through a few more linear layers - `RobertaIntermediate` and `RobertaOutput`. This is also the end of the pretrained model.\n",
    "\n",
    "When we initiate the modal for binary sentiment classification, a `RobertaClassificationHead` is attached. Its weights are randomly initiated (not pre-trained), and we need to train them before running inferences.\n",
    "\n",
    "The building block for all of the above modules are `nn.Linear` layers. \n",
    "\n",
    "For a linear layer with `in_features = 768` and `out_features = 768`, there will be a weight matrix `W` of shape `[768, 768]` and an optional `bias` of shape `[768]`. In a full fine-tuning, all of these parameters will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LoRA?\n",
    "\n",
    "The LoRA paper suggests that we actually do not need to retrain the whole `W` matrix. Particularly, for a matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, we represent the update with a low-rank decomposition:\n",
    "\n",
    "$$\n",
    "W_0 + \\Delta W = W_0 + BA, \\text{ where } B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}, \\text{ and the rank } r \\ll \\min(d, k).\n",
    "$$\n",
    "\n",
    "We will only train $B$ and $A$ during fine-tuning, and keep the pre-trained weights untouched.\n",
    "\n",
    "The original forward pass is $h = W_0 \\mathbf{x}$. With LoRA fine-tuning, the forward pass will be:\n",
    "$$\n",
    "h = W_0 \\mathbf{x} + \\Delta W \\mathbf{x} = W_0 \\mathbf{x} + B A \\mathbf{x}\n",
    "$$\n",
    "\n",
    "In our specfic case, the author chose the rank $\\mathbf{r}$ to be 8. Therefore, $B$ and $A$ will have shape `[768, 8]` and `[8, 768]` respectively.\n",
    "\n",
    "This method result in much less parameter to train:\n",
    "\n",
    "* Original trainable parameters: $768 * 768 = 589,824$\n",
    "* LoRA trainable parameters: $768 * 8 + 8 * 768 = 12,288$\n",
    "\n",
    "### Where will LoRA be applied?\n",
    "\n",
    "This will be applied on the **attention modules only**. Particularly, we only modify the `query` and the `value` layers of the attention modules, while keeping the other parts intact.\n",
    "\n",
    "### Implementation details:\n",
    "\n",
    "From the model architecture above, the attention module in RoBERTa-base is `RobertaSdpaSelfAttention`. This module has the following inheritance chain:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    " <code>RobertaSdpaSelfAttention</code> => <code>RobertaSelfAttention</code> => <code>nn.Module</code>\n",
    "</div>\n",
    "\n",
    "You can take a look at the code [here](https://github.com/huggingface/transformers/blob/5fa35344755d8d9c29610b57d175efd03776ae9e/src/transformers/models/roberta/modeling_roberta.py#L287).\n",
    "\n",
    "`RobertaSelfAttention` is where the `query` and `value` linear layers are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugging in the custom LoRA attention module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.lora_rank = 8\n",
    "model.config.lora_alpha = 8\n",
    "\n",
    "model_original.config.lora_rank = 8\n",
    "model_original.config.lora_alpha = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRobertaSdpaSelfAttention(\n",
       "  (query): LoRALinear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): LoRALinear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LoRARobertaSdpaSelfAttention(model_original.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model.roberta.encoder.layer)):\n",
    "    lora_attention = LoRARobertaSdpaSelfAttention(model.config).to(device)\n",
    "\n",
    "    # update query layer\n",
    "    lora_attention.query.weight = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.query.weight))\n",
    "    lora_attention.query.bias = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.query.bias))\n",
    "\n",
    "    # update key layer\n",
    "    lora_attention.key.weight = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.key.weight))\n",
    "    lora_attention.key.bias = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.key.bias))\n",
    "\n",
    "    # update value layer\n",
    "    lora_attention.value.weight = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.value.weight))\n",
    "    lora_attention.value.bias = Parameter(torch.clone(model.roberta.encoder.layer[idx].attention.self.value.bias))\n",
    " \n",
    "    model.roberta.encoder.layer[idx].attention.self = lora_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "\n",
    "We want the result from the original model and the LoRA model to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5241/778549105.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load('./datasets/train_dataset.pth')\n",
      "/tmp/ipykernel_5241/778549105.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load('./datasets/val_dataset.pth')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.load('./datasets/train_dataset.pth')\n",
    "val_dataset = torch.load('./datasets/val_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7031794318131038, 0.49107142857142855)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss_and_accuracy(model=model_original, dataset=val_dataset, device=device, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7031794318131038, 0.49107142857142855)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss_and_accuracy(model=model, dataset=val_dataset, device=device, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures are equal. Very nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LoRA model\n",
    "\n",
    "We only train the LoRA parameters, so we are freezing all the params and then unfreeze the LoRA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter count:\n",
      "Original model: 124,647,170\n",
      "LoRA model: 294,912\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable parameter count:\")\n",
    "print(f\"Original model: {sum(p.numel() for p in model_original.parameters() if p.requires_grad):,}\")\n",
    "print(f\"LoRA model: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ~0.3M trainable parameters, which matches the figure mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= RUN PARAMETERS: ===============\n",
      "Learning rate: 5.0e-04, batch size: 16\n",
      "Epoch [1/60]: Train loss: 0.0205, Validation loss: 0.2014, Accuracy: 0.9275\n",
      "Epoch [2/60]: Train loss: 0.0161, Validation loss: 0.1940, Accuracy: 0.9263\n",
      "Epoch [3/60]: Train loss: 0.0143, Validation loss: 0.2066, Accuracy: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/60]: Train loss: 0.0144, Validation loss: 0.2266, Accuracy: 0.9286\n",
      "Epoch [5/60]: Train loss: 0.0107, Validation loss: 0.1889, Accuracy: 0.9408\n",
      "Epoch [6/60]: Train loss: 0.0104, Validation loss: 0.1969, Accuracy: 0.9364\n",
      "Epoch [7/60]: Train loss: 0.0096, Validation loss: 0.2350, Accuracy: 0.9342\n",
      "Epoch [8/60]: Train loss: 0.0088, Validation loss: 0.2161, Accuracy: 0.9330\n",
      "Epoch [9/60]: Train loss: 0.0080, Validation loss: 0.2200, Accuracy: 0.9397\n",
      "Epoch [10/60]: Train loss: 0.0068, Validation loss: 0.2138, Accuracy: 0.9319\n",
      "Epoch [11/60]: Train loss: 0.0073, Validation loss: 0.2196, Accuracy: 0.9330\n",
      "Epoch [12/60]: Train loss: 0.0061, Validation loss: 0.2138, Accuracy: 0.9330\n",
      "Epoch [13/60]: Train loss: 0.0067, Validation loss: 0.2139, Accuracy: 0.9364\n",
      "Epoch [14/60]: Train loss: 0.0057, Validation loss: 0.2043, Accuracy: 0.9442\n",
      "Epoch [15/60]: Train loss: 0.0053, Validation loss: 0.1953, Accuracy: 0.9420\n",
      "Epoch [16/60]: Train loss: 0.0056, Validation loss: 0.2163, Accuracy: 0.9319\n",
      "Epoch [17/60]: Train loss: 0.0054, Validation loss: 0.2472, Accuracy: 0.9330\n",
      "Epoch [18/60]: Train loss: 0.0041, Validation loss: 0.2163, Accuracy: 0.9319\n",
      "Epoch [19/60]: Train loss: 0.0045, Validation loss: 0.2151, Accuracy: 0.9353\n",
      "Epoch [20/60]: Train loss: 0.0040, Validation loss: 0.2365, Accuracy: 0.9420\n",
      "Epoch [21/60]: Train loss: 0.0034, Validation loss: 0.2368, Accuracy: 0.9420\n",
      "Epoch [22/60]: Train loss: 0.0041, Validation loss: 0.2434, Accuracy: 0.9319\n",
      "Epoch [23/60]: Train loss: 0.0035, Validation loss: 0.2458, Accuracy: 0.9353\n",
      "Epoch [24/60]: Train loss: 0.0038, Validation loss: 0.2005, Accuracy: 0.9475\n",
      "Epoch [25/60]: Train loss: 0.0031, Validation loss: 0.2552, Accuracy: 0.9342\n",
      "Epoch [26/60]: Train loss: 0.0027, Validation loss: 0.2615, Accuracy: 0.9386\n",
      "Epoch [27/60]: Train loss: 0.0025, Validation loss: 0.2356, Accuracy: 0.9408\n",
      "Epoch [28/60]: Train loss: 0.0027, Validation loss: 0.2412, Accuracy: 0.9353\n",
      "Epoch [29/60]: Train loss: 0.0026, Validation loss: 0.2456, Accuracy: 0.9353\n",
      "Epoch [30/60]: Train loss: 0.0026, Validation loss: 0.2584, Accuracy: 0.9342\n",
      "Epoch [31/60]: Train loss: 0.0023, Validation loss: 0.2569, Accuracy: 0.9364\n",
      "Epoch [32/60]: Train loss: 0.0026, Validation loss: 0.2515, Accuracy: 0.9364\n",
      "Epoch [33/60]: Train loss: 0.0023, Validation loss: 0.1909, Accuracy: 0.9498\n",
      "Epoch [34/60]: Train loss: 0.0019, Validation loss: 0.2507, Accuracy: 0.9375\n",
      "Epoch [35/60]: Train loss: 0.0022, Validation loss: 0.2363, Accuracy: 0.9375\n",
      "Epoch [36/60]: Train loss: 0.0014, Validation loss: 0.2399, Accuracy: 0.9397\n",
      "Epoch [37/60]: Train loss: 0.0015, Validation loss: 0.2605, Accuracy: 0.9420\n",
      "Epoch [38/60]: Train loss: 0.0018, Validation loss: 0.2624, Accuracy: 0.9408\n",
      "Epoch [39/60]: Train loss: 0.0013, Validation loss: 0.2363, Accuracy: 0.9464\n",
      "Epoch [40/60]: Train loss: 0.0014, Validation loss: 0.2570, Accuracy: 0.9420\n",
      "Epoch [41/60]: Train loss: 0.0017, Validation loss: 0.2689, Accuracy: 0.9330\n",
      "Epoch [42/60]: Train loss: 0.0013, Validation loss: 0.2472, Accuracy: 0.9453\n",
      "Epoch [43/60]: Train loss: 0.0013, Validation loss: 0.2497, Accuracy: 0.9420\n",
      "Epoch [44/60]: Train loss: 0.0013, Validation loss: 0.2493, Accuracy: 0.9420\n",
      "Epoch [45/60]: Train loss: 0.0012, Validation loss: 0.2632, Accuracy: 0.9431\n",
      "Epoch [46/60]: Train loss: 0.0014, Validation loss: 0.2516, Accuracy: 0.9442\n",
      "Epoch [47/60]: Train loss: 0.0011, Validation loss: 0.2680, Accuracy: 0.9397\n",
      "Epoch [48/60]: Train loss: 0.0012, Validation loss: 0.2466, Accuracy: 0.9464\n",
      "Epoch [49/60]: Train loss: 0.0011, Validation loss: 0.2597, Accuracy: 0.9397\n",
      "Epoch [50/60]: Train loss: 0.0013, Validation loss: 0.2608, Accuracy: 0.9464\n",
      "Epoch [51/60]: Train loss: 0.0009, Validation loss: 0.2447, Accuracy: 0.9442\n",
      "Epoch [52/60]: Train loss: 0.0010, Validation loss: 0.2518, Accuracy: 0.9453\n",
      "Epoch [53/60]: Train loss: 0.0008, Validation loss: 0.2615, Accuracy: 0.9498\n",
      "Epoch [54/60]: Train loss: 0.0007, Validation loss: 0.2498, Accuracy: 0.9498\n",
      "Epoch [55/60]: Train loss: 0.0008, Validation loss: 0.2596, Accuracy: 0.9464\n",
      "Epoch [56/60]: Train loss: 0.0009, Validation loss: 0.2685, Accuracy: 0.9453\n",
      "Epoch [57/60]: Train loss: 0.0010, Validation loss: 0.2882, Accuracy: 0.9408\n",
      "Epoch [58/60]: Train loss: 0.0006, Validation loss: 0.2610, Accuracy: 0.9464\n",
      "Epoch [59/60]: Train loss: 0.0007, Validation loss: 0.2604, Accuracy: 0.9464\n",
      "Epoch [60/60]: Train loss: 0.0008, Validation loss: 0.2713, Accuracy: 0.9397\n",
      "Best accuracy is 0.9498 at epoch 33\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "batch_size = 16\n",
    "learning_rate = 5e-4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "warmup_ratio = 0.06\n",
    "warmup_steps = warmup_ratio * total_steps\n",
    "\n",
    "print('========= RUN PARAMETERS: ===============')\n",
    "print(f'Learning rate: {learning_rate:.1e}, batch size: {batch_size}')\n",
    "\n",
    "optimizer = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer=optimizer,\n",
    "    start_factor=0.05,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_steps,\n",
    ")\n",
    "decay_scheduler = LinearLR(\n",
    "    optimizer=optimizer,\n",
    "    start_factor=1.0,\n",
    "    end_factor=0.0,\n",
    "    total_iters=total_steps - warmup_steps,\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer=optimizer,\n",
    "    schedulers=[warmup_scheduler, decay_scheduler],\n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "\n",
    "# training loop\n",
    "train_losses, val_losses, accuracies = [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "        # forward\n",
    "        logits = model(**x).logits\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "    # get train loss\n",
    "    train_loss, _ = get_loss_and_accuracy(\n",
    "        model=model,\n",
    "        dataset=train_dataset,\n",
    "        device=device,\n",
    "        eval_ratio=0.1 # evaluate on 10% of train data\n",
    "    )\n",
    "\n",
    "    # Get validation loss and accuracy\n",
    "    val_loss, val_accuracy = get_loss_and_accuracy(\n",
    "        model=model,\n",
    "        dataset=val_dataset,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(f'Best accuracy is {max(accuracies):.4f} at epoch {accuracies.index(max(accuracies)) + 1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
